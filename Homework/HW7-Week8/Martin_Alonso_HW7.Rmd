---
title: "Martin_Alonso_HW7"
author: "Martin Alonso"
date: "November 25, 2018"
output: 
  word_document:
    fig_width: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# HW7 Instructions
In this homework, you will use SVMs, kNN, and Random Forest algorithms for handwriting recognition, and compare their performance with the na√Øve Bayes and decision tree models you built in previous week.

## Loading the packages and data
We'll start by loading the `caret`, `e1071`, and `dplyr` packages that will be used. Though the original exercise involved submitting to Kaggle, this is no longer necessary, so we'll just work with the training set. 
Due to the size of the dataset, we will only explore part of it so as to not saturate the output. 

```{r tidy=TRUE}
# Load the required packages.
require(caret)
require(e1071)
require(rpart)
require(dplyr)
require(stringr)
require(randomForest)

# Load the train dataset. 
dataSet <- read.csv('Kaggle-digit-train.csv')

# Explore the dimension of the dataset, and the structure and summary of the first ten rows. 
dim(dataSet)

str(dataSet[, 1:10])

summary(dataSet[, 1:10])

# Let's also check what the first 5 rows of the dataset look like.
head(dataSet[, 1:10], n = 5)

```

Now that we have checked what the dataset look like, it's time to do some additional processing and then train the models. 

# The Datasets. 
For this exercise, we'll work on two new models: k-Nearest Neighbours (kNN) and Support Vector Machines (SVM). We'll also work on a third model - Random Forest - which is an expansion of the decision tree algorithm we have previously worked on. 
Also, due to the size of the training set, we'll only work with 30 percent of the dataset, setting the seeds and selecting two-thirds of this subset to serve as our training set and the remaining third to serve as our testing set. All these subsets will be chosen at random.  

## Data preprocessing
Furthermore, because there are so many variables, we'll make sure that we work with a select number of objects. Just to make sure that we don't eliminate unneccesary columns, only columns where there is no variance will be eliminated. 
```{r tidy = TRUE}
# Replace NAs with 0, count number of columns, and create a list that will populate the columns that sum 0. 
dataSet[is.na(dataSet)] <- 0
cols <- ncol(dataSet)
vars <- list()

# Loop over the variables to find which present no variance. 
for(i in 2:cols){
  colVar <- var(dataSet[[i]])
  if(colVar == 0){
    vars <- append(vars, i)
  }
}

# Drop the columns that have no variance in both the test and training set. 
colsToDrop <- unlist(vars)

dataSet <- dataSet[, -colsToDrop]

# With the columns now selected, we'll now choose 20 percent of both the training and testing datasets. 
set.seed(1024)

split <- sample(nrow(dataSet), nrow(dataSet) * .3)
dataSubset <- dataSet[split, ]

# Now to select the training and testing sets from this subset. 
set.seed(760)

trainSplit <- sample(nrow(dataSubset), nrow(dataSubset) * 2/3)
trainSet <- dataSubset[trainSplit, ]
testSet <- dataSubset[-trainSplit, ]
```

After selecting the columns, we'll end up working with 8,400 records to train the model and 4,200 records to test the model.

# Training and testing the models. 

## k-Nearest Neighbours
Starting off with kNN, we'll build the model using two-thirds of the training set and then use the remaining third of the same data to test the results. 
Just to avoid redoing this process, we'll set the seed for the split and work from there. 

```{r tidy = TRUE}
# We'll first start by converting the label variable into a factor. Because we are building a classifier that will determine the probability that a observation belongs to a certain number, we need to convert the labels into factors that can then be converted to variable names
trainSet$label <- factor(paste0('X', trainSet$label), levels = c('X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9'))
testSet$label <- factor(paste0('X', testSet$label), levels = c('X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9'))

# Now to start creating the kNN. We'll do three-fold CV as in the previous exercise. 
set.seed(239)
x <- trainControl(method = 'repeatedcv', repeats = 3, classProbs = TRUE)
knnTrain <- train(label ~ ., data = trainSet, method = 'knn', preProcess = c('center', 'scale'), trControl = x, metric = 'ROC', tuneLength = 3)

# Model summary
knnTrain
plot(knnTrain)

# Given these numbers, it appears that the model is extremely good at predicting the class label. Let's run the model on the test set. 
knnPred <- predict(knnTrain, testSet, type = 'prob')
knnPred <- as.data.frame(knnPred)

# Select maximum prediction.
# We subtract 1 because we are showing the class label (i.e 0 is 1, 1 is 2, and so on.)
knnPredictedValues <- data.frame(apply(knnPred, 1, which.max) - 1) 
colnames(knnPredictedValues) <- 'prediction'

# Compare to actual values
knnResults <- testSet %>% select(label) %>% mutate(real = str_remove(label, 'X')) %>% bind_cols(knnPredictedValues) %>% mutate(real = as.factor(real), prediction = as.factor(prediction))

confusionMatrix(knnResults$real, knnResults$prediction)
```

The model takes a long time to run - training this model took almost two hours! - but the end results show that it is highly accurate when predicting class labels. We can see that for both the training and test sets, the accuracy is slightly over 90 percent which, so far, are the best results we have seen. 

## Support Vector Machines

Now, our attention will turn to SVMs. Since the data has already been processed, we will train the model. 

```{r tidy = TRUE}
# Run the model using a linear kernel to classify the data. We'll do 3 k-fold cross validatiion. 
svmTrain <- svm(label ~ ., data = trainSet, type = 'C', kernel = 'linear', cross = 3, probability = TRUE)

summary(svmTrain)
```

These results are really amazing! SVM has a higher degree of accuracy within the training set than what kNN showed originally. 
Now to run the test set on the model. 

```{r tidy = TRUE}
svmPred <- predict(svmTrain, testSet, type = 'prob')
svmPred <- as.data.frame(svmPred)
colnames(svmPred) <- 'results'

# Build the data frame to compare the results. 
svmResults <- testSet %>% select(label) %>% bind_cols(svmPred) %>% mutate(real = factor(as.character(str_remove(label, 'X'))), prediction = factor(as.character(str_remove(results, 'X'))))

confusionMatrix(svmResults$real, svmResults$prediction)

```

The model's consistency is very impressive. It manages to also predict at a high accuracy. 

## Random Forest

Let's turn to the final model that we will work on: random forest. 

```{r tidy = TRUE}
x <- trainControl(method = 'repeatedcv', number = 3, repeats = 3)
rfTrain <- train(label ~ ., data = trainSet, method = 'rf', metric = 'Accuracy', trControl = x, type = 'C')

rfTrain

# With these results, we see that the train has a minimum accuracy on two variables sampled, but this increases once we reach 37.
```

With 94.6 percent accuracy, this is by far the superior model when comparing the three models we've built and the results according to the training set. But the most important thing is to compare the results of the testing set. 

```{r tidy = TRUE}
# Test the random forest model. 
rfPred <- predict(rfTrain, testSet, type = 'prob')
rfPred <- as.data.frame(rfPred)

rfPredictedValues <- data.frame(apply(rfPred, 1, which.max) - 1) 
colnames(rfPredictedValues) <- 'results'

# Random forest actual and predicted labels, and confusion matrix to comparte the model's prediciton capabilities.
rfResults <- testSet %>% select(label) %>% bind_cols(rfPredictedValues) %>% mutate(real = factor(as.character(str_remove(label, 'X'))), prediction = factor(results))

confusionMatrix(rfResults$real, rfResults$prediction)
```

So results are on par with what was expected: 94.7 percent accuracy, by far the best out of these three models. 

# Final comments
All three models tested this week present high degrees of accuracy: all of them are accurate over 90 percent of the time. However, there are some caveats to bear in mind. First, the size of the data is immense, and so, we had to cut corners. We dismissed using the original test set as it did not have class labels, which we would've only been able to actually test in Kaggle. Because of this, we worked only on the training dataset. 
However, because this dataset was over 40,000 records long, we also limited working with 30 percent of this - with all observations chosen at random. Since this was done prior to any splitting into a new training and testing set, there is no chance for observations to appear in both sets. What is concerning is that maybe this would probably cause the models to overfit because of the limited size of the training sample. This could be ammended by using a larger set and adding more computing power. 

Despite all this, these initial results are satisfying and merit additional investigation towards finer parameter tuning and slowly add more observations to the training set to produce a better model. 