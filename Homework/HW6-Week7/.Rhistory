(head(DigitDF_Test_noLabel))
# decision tree modeling with Train
dt_Train <- rpart(label ~., data = DigitDF_Train, method="class", control=rpart.control(cp=0, minsplit=4, maxdepth=25))
rpart.plot(dt_Train, extra=104, box.palette="GnBu", cex=NULL, branch.lty=3, shadow.col="gray", nn=TRUE)
dt_Pred <- predict(dt_Train, DigitDF_Test_noLabel, type="class") # maybe withoutlabels?
#Testing accurancy of decision tree with Kaggle train data sub set
(confusionMatrix(dt_Pred, DigitDF_Test$label))
# Accumulate results from each fold, if you like
AllResultsDT <- c(AllResultsDT,dt_Pred)
AllLabelsDT <- c(AllLabelsDT, DigitDF_Test_justLabel)
}
### end crossvalidation -- present results for all folds
dt_table <- table(unlist(AllResultsDT),unlist(AllLabelsDT))
# test accuracy
print(paste('Accuracy for CV Decision Trees', sum(diag(dt_table)) / sum(dt_table)))
Para_DTreeDF <- rbind(Para_DTreeDF, c(4, 25, sum(diag(dt_table)) / sum(dt_table)))
ind <- 1
while (ind<-nrow(Para_DTreeDF))
{
print(paste('Accuracy for Decision Tree with Min Split =',
Para_DtreeDF[ind,1]), "and Max Depth =", Para_DtreeDF[ind,2], "is:", Para_DtreeDF[ind,3])
ind<-ind+1
}
ind <- 1
while (ind<-nrow(Para_DTreeDF))
{
print(paste('Accuracy for Decision Tree with Min Split =',
Para_DTreeDF[ind,1]), "and Max Depth =", Para_DTreeDF[ind,2], "is:", Para_DTreeDF[ind,3])
ind<-ind+1
}
Para_DTreeDF[1:1]
ind <- 1
while (ind<-nrow(Para_DTreeDF))
{
print(paste('Accuracy for Decision Tree with Min Split =',
Para_DTreeDF[ind:1]), "and Max Depth =", Para_DTreeDF[ind:2], "is:", Para_DTreeDF[ind:3])
ind<-ind+1
}
ind
ind <- 1
print(paste('Accuracy for Decision Tree with Min Split =',
Para_DTreeDF[ind:1]), "and Max Depth =", Para_DTreeDF[ind:2], "is:", Para_DTreeDF[ind:3])
print(paste('Accuracy for Decision Tree with Min Split ='))
print(paste('Accuracy for Decision Tree with Min Split =', Para_DTreeDF[ind:1]))
print(paste('Accuracy for Decision Tree with Min Split =', Para_DTreeDF[ind:1], Para_DTreeDF[ind:2]))
print(paste('Accuracy for Decision Tree with Min Split =', Para_DTreeDF[ind:1], Para_DTreeDF[ind:2]))
ind
Para_DTreeDF[ind:2]
Para_DTreeDF[ind,2]
rownames(Para_DTreeDF) < c("")
ParaT <- ParaDTreeDF
ParaT <- Para_DTreeDF
rownames(Para_DTreeDF) <- c("")
Para_DTreeDF[1:1]
Para_DTreeDF[2:1]
Para_DTreeDF[2:2]
Para_DTreeDF
ind <- 1
while (ind<-nrow(Para_DTreeDF))
{
print(paste('Accuracy for Decision Tree with Min Split =',
Para_DTreeDF[ind,1]), "and Max Depth =", Para_DTreeDF[ind,2], "is:", Para_DTreeDF[ind,3])
ind<-ind+1
}
ind
ind <- 1
ind <- 1
while (ind<-nrow(Para_DTreeDF))
{
print(paste('Accuracy for Decision Tree with Min Split =',
Para_DTreeDF[ind,1]), 'and Max Depth =', Para_DTreeDF[ind,2], 'is:', Para_DTreeDF[ind,3])
ind<-ind+1
}
Para_DTreeDF[1,1]
Para_DTreeDF[1,2]
Para_DTreeDF[1,3]
rownames(Para_DTreeDF) <- c()
colnames(Para_DTreeDF) <- c("minsplit", "maxdepth", "accuracy")
View(Para_DTreeDF)
View(Para_DTreeDF)
Para_DTreeDF$accuracy[1]
Para_DTreeDF$accuracy[1,]
Para_DTreeDF$accuracy[,1]
data.frame(Para_DTreeDF)
Para_DTreeDF <- data.frame(Para_DTreeDF)
Para_DTreeDF$accuracy[,1]
Para_DTreeDF$accuracy[1]
Para_DTreeDF$depth[1]
ind <- 1
while (ind<-nrow(Para_DTreeDF))
{
print(paste('Accuracy for Decision Tree with Min Split =',
Para_DTreeDF$minsplit[ind]), 'and Max Depth =', Para_DTreeDF$maxdepth[ind], 'is:', Para_DTreeDF$accuracy[ind])
ind<-ind+1
}
nrow(Para_DTreeDF
)
ind <- 1
while (ind < nrow(Para_DTreeDF))
{
print(paste('Accuracy for Decision Tree with Min Split =',
Para_DTreeDF$minsplit[ind]), 'and Max Depth =', Para_DTreeDF$maxdepth[ind], 'is:', Para_DTreeDF$accuracy[ind])
ind<-ind+1
}
ind <- 1
ind < nrow(Para_DTreeDF)
print(paste('Accuracy for Decision Tree with Min Split =',
Para_DTreeDF$minsplit[ind]), 'and Max Depth =', Para_DTreeDF$maxdepth[ind], 'is:', Para_DTreeDF$accuracy[ind])
install.packages("mlr")
N <- nrow(DigitDF)
## Number of desired splits
kfolds <- 10
## Generate indices of holdout observations
## Note if N is not a multiple of folds you will get a warning, but is OK.
holdout <- split(sample(1:N), 1:kfolds)
#Run training and Testing for each of the k-folds
# naive bayes for kfolds
AllResults<-list()
AllLabels<-list()
for (k in 1:kfolds){
DigitDF_Test <- DigitDF[holdout[[k]], ]
DigitDF_Train=DigitDF[-holdout[[k]], ]
## View the created Test and Train sets
(head(DigitDF_Train))
(table(DigitDF_Test$Label))
## Make sure you take the labels out of the testing data
(head(DigitDF_Test))
DigitDF_Test_noLabel<-DigitDF_Test[-c(1)]
DigitDF_Test_justLabel<-DigitDF_Test$label
(head(DigitDF_Test_noLabel))
#### Naive Bayes prediction ussing e1071 package
#Naive Bayes Train model
train_naibayes<-naiveBayes(label~., data=DigitDF_Train, na.action = na.pass)
train_naibayes
#summary(train_naibayes)
#Naive Bayes model Prediction
nb_Pred <- predict(train_naibayes, DigitDF_Test_noLabel)
nb_Pred
#Testing accurancy of naive bayes model with Kaggle train data sub set
(confusionMatrix(nb_Pred, DigitDF_Test$label))
# Accumulate results from each fold, if you like
AllResults<- c(AllResults,nb_Pred)
AllLabels<- c(AllLabels, DigitDF_Test_justLabel)
}
### end crossvalidation -- present results for all folds
nb_table <- table(unlist(AllResults),unlist(AllLabels))
nb_table
# test accuracy
print(paste('Accuracy for CV Naive Bayes', sum(diag(nb_table)) / sum(nb_table)))
NB_object<- naive_bayes(label~., data=DigitDF)
NB_prediction<-predict(NB_object, DigitDF_Test_noLabel, type = c("class"))
head(predict(NB_object, DigitDF_Test_noLabel, type = "prob"))
NB_Table <- table(NB_prediction,DigitDF_Test_justLabel)
sum(diag(NB_Table)) / sum(NB_Table)
print(paste('Accuracy for CV Naive Bayes', sum(diag(NB_Table)) / sum(NB_Table)))
#Loading the library
library(mlr)
#Loading the library
library(mlr)
#Create a classification task for learning on entire DigitDF dataset (not 10-fold)
task <- makeClassifTask(data = DigitDF, target = "label")
#Initialize the Naive Bayes classifier
selected_model <- makeLearner("classif.naiveBayes")
#Train the model
NB_mlr <- train(selected_model, task)
#Read the model learned
NB_mlr$learner.model
#Predict on the dataset without passing the target feature
predictions_mlr = as.data.frame(predict(NB_mlr, newdata = DigitDF_Test_noLabel))
##Confusion matrix to check accuracy
table(predictions_mlr[,1],DigitDF_Test$label)
##Confusion matrix to check accuracy
NB_mlr_table <- table(predictions_mlr[,1],DigitDF_Test$label)
print(paste('Accuracy for MLR Naive Bayes', sum(diag(NB_mlr_table)) / sum(NB_mlr_table)))
startTime <- proc.time()
randF <- randomForest(train_orig[-1], train_orig_labels, xtest=test_orig,
ntree=25, importance=TRUE, keep.forest=TRUE)
proc.time() - startTime
startTime <- proc.time()
predictions <- data.frame(ImageID=1:nrow(test_orig),
label=levels(train_orig_labels)[randF$test$predicted])
proc.time() - startTime
predictions
N <- nrow(DigitDF)
## Number of desired splits
kfolds <- 10
## Generate indices of holdout observations
## Note if N is not a multiple of folds you will get a warning, but is OK.
# these are sets of observations to use as samples - there are 10 sets
holdout <- split(sample(1:N), 1:kfolds)
head(holdout)
dtree <- function (traindf, testdf, c_p, min_split, max_depth, graph_pall)
{
# build the model
Dtree <- rpart(label ~., data = traindf, method="class", control=rpart.control(cp=c_p, minsplit=min_split, maxdepth=max_depth))
# reflect the error
rsq.rpart(Dtree)
# plot the tree
rpart.plot(Dtree, extra=104, box.palette="GnBu", branch.lty=3, shadow.col="gray", nn=TRUE)
return(Dtree)
}
Para_DTreeDF <- NULL
#Run training and Testing for each of the k-folds
# decision trees for kfolds
AllResultsDT <-list()
AllLabelsDT <-list()
startTime <- proc.time()
for (k in 1:kfolds)
{
# grab a set to be Test set
DigitDF_Test <- DigitDF[holdout[[k]], ]
DigitDF_Train=DigitDF[-holdout[[k]], ]
## View the created Test and Train sets
(head(DigitDF_Train))
(table(DigitDF_Test$Label))
## Make sure you take the labels out of the testing data
(head(DigitDF_Test))
DigitDF_Test_noLabel<-DigitDF_Test[-c(1)]
DigitDF_Test_justLabel<-DigitDF_Test$label
(head(DigitDF_Test_noLabel))
# decision tree modeling with Train
dt_Train <- rpart(label ~., data = DigitDF_Train, method="class", control=rpart.control(cp=0, minsplit=2, maxdepth=4))
rpart.plot(dt_Train, extra=104, box.palette="GnBu", cex=NULL, branch.lty=3, shadow.col="gray", nn=TRUE)
dt_Pred <- predict(dt_Train, DigitDF_Test_noLabel, type="class") # maybe withoutlabels?
#Testing accurancy of decision tree with Kaggle train data sub set
(confusionMatrix(dt_Pred, DigitDF_Test$label))
# Accumulate results from each fold, if you like
AllResultsDT <- c(AllResultsDT,dt_Pred)
AllLabelsDT <- c(AllLabelsDT, DigitDF_Test_justLabel)
}
proc.time() - startTime
### end crossvalidation -- present results for all folds
dt_table <- table(unlist(AllResultsDT),unlist(AllLabelsDT))
# test accuracy
print(paste('Accuracy for CV Decision Trees', sum(diag(dt_table)) / sum(dt_table)))
# build a matrix with the parameters and accuracy so we can use this to determine the best tree
# Run each of the above changing the minsplit and maxdepth
Para_DTreeDF <- rbind(Para_DTreeDF, c(2, 4, sum(diag(dt_table)) / sum(dt_table)))
AllResultsDT <-list()
AllLabelsDT <-list()
startTime <- proc.time()
for (k in 1:kfolds)
{
# grab a set to be Test set
DigitDF_Test <- DigitDF[holdout[[k]], ]
DigitDF_Train=DigitDF[-holdout[[k]], ]
## View the created Test and Train sets
(head(DigitDF_Train))
(table(DigitDF_Test$Label))
## Make sure you take the labels out of the testing data
(head(DigitDF_Test))
DigitDF_Test_noLabel<-DigitDF_Test[-c(1)]
DigitDF_Test_justLabel<-DigitDF_Test$label
(head(DigitDF_Test_noLabel))
# decision tree modeling with Train
dt_Train <- rpart(label ~., data = DigitDF_Train, method="class", control=rpart.control(cp=0, minsplit=3, maxdepth=5))
rpart.plot(dt_Train, extra=104, box.palette="GnBu", cex=NULL, branch.lty=3, shadow.col="gray", nn=TRUE)
dt_Pred <- predict(dt_Train, DigitDF_Test_noLabel, type="class") # maybe withoutlabels?
#Testing accurancy of decision tree with Kaggle train data sub set
(confusionMatrix(dt_Pred, DigitDF_Test$label))
# Accumulate results from each fold, if you like
AllResultsDT <- c(AllResultsDT,dt_Pred)
AllLabelsDT <- c(AllLabelsDT, DigitDF_Test_justLabel)
}
proc.time() - startTime
### end crossvalidation -- present results for all folds
dt_table <- table(unlist(AllResultsDT),unlist(AllLabelsDT))
# test accuracy
print(paste('Accuracy for CV Decision Trees', sum(diag(dt_table)) / sum(dt_table)))
Para_DTreeDF <- rbind(Para_DTreeDF, c(3, 5, sum(diag(dt_table)) / sum(dt_table)))
#Run training and Testing for each of the k-folds
# decision trees for kfolds
AllResultsDT <-list()
AllLabelsDT <-list()
startTime <- proc.time()
for (k in 1:kfolds)
{
# grab a set to be Test set
DigitDF_Test <- DigitDF[holdout[[k]], ]
DigitDF_Train=DigitDF[-holdout[[k]], ]
## View the created Test and Train sets
(head(DigitDF_Train))
(table(DigitDF_Test$Label))
## Make sure you take the labels out of the testing data
(head(DigitDF_Test))
DigitDF_Test_noLabel<-DigitDF_Test[-c(1)]
DigitDF_Test_justLabel<-DigitDF_Test$label
(head(DigitDF_Test_noLabel))
# decision tree modeling with Train
dt_Train <- rpart(label ~., data = DigitDF_Train, method="class", control=rpart.control(cp=0, minsplit=4, maxdepth=4))
rpart.plot(dt_Train, extra=104, box.palette="GnBu", cex=NULL, branch.lty=3, shadow.col="gray", nn=TRUE)
dt_Pred <- predict(dt_Train, DigitDF_Test_noLabel, type="class") # maybe withoutlabels?
#Testing accurancy of decision tree with Kaggle train data sub set
(confusionMatrix(dt_Pred, DigitDF_Test$label))
# Accumulate results from each fold, if you like
AllResultsDT <- c(AllResultsDT,dt_Pred)
AllLabelsDT <- c(AllLabelsDT, DigitDF_Test_justLabel)
}
proc.time() - startTime
### end crossvalidation -- present results for all folds
dt_table <- table(unlist(AllResultsDT),unlist(AllLabelsDT))
# test accuracy
print(paste('Accuracy for CV Decision Trees', sum(diag(dt_table)) / sum(dt_table)))
Para_DTreeDF <- rbind(Para_DTreeDF, c(4, 4, sum(diag(dt_table)) / sum(dt_table)))
AllResultsDT <-list()
AllLabelsDT <-list()
startTime <- proc.time()
for (k in 1:kfolds)
{
# grab a set to be Test set
DigitDF_Test <- DigitDF[holdout[[k]], ]
DigitDF_Train=DigitDF[-holdout[[k]], ]
## View the created Test and Train sets
(head(DigitDF_Train))
(table(DigitDF_Test$Label))
## Make sure you take the labels out of the testing data
(head(DigitDF_Test))
DigitDF_Test_noLabel<-DigitDF_Test[-c(1)]
DigitDF_Test_justLabel<-DigitDF_Test$label
(head(DigitDF_Test_noLabel))
# decision tree modeling with Train
dt_Train <- rpart(label ~., data = DigitDF_Train, method="class", control=rpart.control(cp=0, minsplit=4, maxdepth=6))
rpart.plot(dt_Train, extra=104, box.palette="GnBu", cex=NULL, branch.lty=3, shadow.col="gray", nn=TRUE)
dt_Pred <- predict(dt_Train, DigitDF_Test_noLabel, type="class") # maybe withoutlabels?
#Testing accurancy of decision tree with Kaggle train data sub set
(confusionMatrix(dt_Pred, DigitDF_Test$label))
# Accumulate results from each fold, if you like
AllResultsDT <- c(AllResultsDT,dt_Pred)
AllLabelsDT <- c(AllLabelsDT, DigitDF_Test_justLabel)
}
proc.time() - startTime
### end crossvalidation -- present results for all folds
dt_table <- table(unlist(AllResultsDT),unlist(AllLabelsDT))
# test accuracy
print(paste('Accuracy for CV Decision Trees', sum(diag(dt_table)) / sum(dt_table)))
Para_DTreeDF <- rbind(Para_DTreeDF, c(4, 6, sum(diag(dt_table)) / sum(dt_table)))
AllResultsDT <-list()
AllLabelsDT <-list()
startTime <- proc.time()
for (k in 1:kfolds)
{
# grab a set to be Test set
DigitDF_Test <- DigitDF[holdout[[k]], ]
DigitDF_Train=DigitDF[-holdout[[k]], ]
## View the created Test and Train sets
(head(DigitDF_Train))
(table(DigitDF_Test$Label))
## Make sure you take the labels out of the testing data
(head(DigitDF_Test))
DigitDF_Test_noLabel<-DigitDF_Test[-c(1)]
DigitDF_Test_justLabel<-DigitDF_Test$label
(head(DigitDF_Test_noLabel))
# decision tree modeling with Train
dt_Train <- rpart(label ~., data = DigitDF_Train, method="class", control=rpart.control(cp=0, minsplit=4, maxdepth=8))
rpart.plot(dt_Train, extra=104, box.palette="GnBu", cex=NULL, branch.lty=3, shadow.col="gray", nn=TRUE)
dt_Pred <- predict(dt_Train, DigitDF_Test_noLabel, type="class") # maybe withoutlabels?
#Testing accurancy of decision tree with Kaggle train data sub set
(confusionMatrix(dt_Pred, DigitDF_Test$label))
# Accumulate results from each fold, if you like
AllResultsDT <- c(AllResultsDT,dt_Pred)
AllLabelsDT <- c(AllLabelsDT, DigitDF_Test_justLabel)
}
proc.time() - startTime
### end crossvalidation -- present results for all folds
dt_table <- table(unlist(AllResultsDT),unlist(AllLabelsDT))
# test accuracy
print(paste('Accuracy for CV Decision Trees', sum(diag(dt_table)) / sum(dt_table)))
startTime <- proc.time()
randF <- randomForest(train_orig[-1], train_orig_labels, xtest=test_orig,
ntree=40, importance=TRUE, keep.forest=TRUE)
proc.time() - startTime
randF # to look at the confusion matrix
randFCM <- randF$confusion
print(paste('Accuracy for RandomForest', sum(diag(randFCM)) / sum(randFCM)))
startTime <- proc.time()
predictions <- data.frame(ImageID=1:nrow(test_orig),
label=levels(train_orig_labels)[randF$test$predicted])
proc.time() - startTime
head(predictions)
str(predictions)
startTime <- proc.time()
randF <- randomForest(train_orig[-1], train_orig_labels, xtest=test_orig,
ntree=50, importance=TRUE, keep.forest=TRUE)
proc.time() - startTime
randF # to look at the confusion matrix
randFCM <- randF$confusion
print(paste('Accuracy for RandomForest', sum(diag(randFCM)) / sum(randFCM)))
startTime <- proc.time()
randF <- randomForest(train_orig[-1], train_orig_labels, xtest=test_orig,
ntree=75, importance=TRUE, keep.forest=TRUE)
proc.time() - startTime
randF # to look at the confusion matrix
randFCM <- randF$confusion
print(paste('Accuracy for RandomForest', sum(diag(randFCM)) / sum(randFCM)))
AllResultsDT <-list()
AllLabelsDT <-list()
startTime <- proc.time()
for (k in 1:kfolds)
{
# grab a set to be Test set
DigitDF_Test <- DigitDF[holdout[[k]], ]
DigitDF_Train=DigitDF[-holdout[[k]], ]
## View the created Test and Train sets
(head(DigitDF_Train))
(table(DigitDF_Test$Label))
## Make sure you take the labels out of the testing data
(head(DigitDF_Test))
DigitDF_Test_noLabel<-DigitDF_Test[-c(1)]
DigitDF_Test_justLabel<-DigitDF_Test$label
(head(DigitDF_Test_noLabel))
# decision tree modeling with Train
dt_Train <- rpart(label ~., data = DigitDF_Train, method="class", control=rpart.control(cp=0, minsplit=2, maxdepth=10))
rpart.plot(dt_Train, extra=104, box.palette="GnBu", cex=NULL, branch.lty=3, shadow.col="gray", nn=TRUE)
dt_Pred <- predict(dt_Train, DigitDF_Test_noLabel, type="class") # maybe withoutlabels?
#Testing accurancy of decision tree with Kaggle train data sub set
(confusionMatrix(dt_Pred, DigitDF_Test$label))
# Accumulate results from each fold, if you like
AllResultsDT <- c(AllResultsDT,dt_Pred)
AllLabelsDT <- c(AllLabelsDT, DigitDF_Test_justLabel)
}
proc.time() - startTime
### end crossvalidation -- present results for all folds
dt_table <- table(unlist(AllResultsDT),unlist(AllLabelsDT))
# test accuracy
print(paste('Accuracy for CV Decision Trees', sum(diag(dt_table)) / sum(dt_table)))
N <- nrow(DigitDF)
## Number of desired splits
kfolds <- 10
## Generate indices of holdout observations
## Note if N is not a multiple of folds you will get a warning, but is OK.
holdout <- split(sample(1:N), 1:kfolds)
#Run training and Testing for each of the k-folds
# naive bayes for kfolds
AllResults<-list()
AllLabels<-list()
startTime <- proc.time()
for (k in 1:kfolds){
DigitDF_Test <- DigitDF[holdout[[k]], ]
DigitDF_Train=DigitDF[-holdout[[k]], ]
## View the created Test and Train sets
(head(DigitDF_Train))
(table(DigitDF_Test$Label))
## Make sure you take the labels out of the testing data
(head(DigitDF_Test))
DigitDF_Test_noLabel<-DigitDF_Test[-c(1)]
DigitDF_Test_justLabel<-DigitDF_Test$label
(head(DigitDF_Test_noLabel))
#### Naive Bayes prediction ussing e1071 package
#Naive Bayes Train model
train_naibayes<-naiveBayes(label~., data=DigitDF_Train, na.action = na.pass)
train_naibayes
#summary(train_naibayes)
#Naive Bayes model Prediction
nb_Pred <- predict(train_naibayes, DigitDF_Test_noLabel)
nb_Pred
#Testing accurancy of naive bayes model with Kaggle train data sub set
(confusionMatrix(nb_Pred, DigitDF_Test$label))
# Accumulate results from each fold, if you like
AllResults<- c(AllResults,nb_Pred)
AllLabels<- c(AllLabels, DigitDF_Test_justLabel)
}
proc.time() - startTime
### end crossvalidation -- present results for all folds
nb_table <- table(unlist(AllResults),unlist(AllLabels))
# test accuracy
print(paste('Accuracy for CV Naive Bayes', sum(diag(nb_table)) / sum(nb_table)))
startTime <- proc.time()
#Create a classification task for learning on entire DigitDF dataset (not 10-fold)
task <- makeClassifTask(data = DigitDF, target = "label")
#Initialize the Naive Bayes classifier
selected_model <- makeLearner("classif.naiveBayes")
#Train the model
NB_mlr <- train(selected_model, task)
#Read the model learned
NB_mlr$learner.model
#Predict on the dataset without passing the target feature
predictions_mlr = as.data.frame(predict(NB_mlr, newdata = DigitDF_Test_noLabel))
##Confusion matrix to check accuracy
NB_mlr_table <- table(predictions_mlr[,1],DigitDF_Test$label)
print(paste('Accuracy for MLR Naive Bayes', sum(diag(NB_mlr_table)) / sum(NB_mlr_table)))
proc.time() - startTime
AllResultsDT <-list()
AllLabelsDT <-list()
startTime <- proc.time()
for (k in 1:kfolds)
{
# grab a set to be Test set
DigitDF_Test <- DigitDF[holdout[[k]], ]
DigitDF_Train=DigitDF[-holdout[[k]], ]
## View the created Test and Train sets
(head(DigitDF_Train))
(table(DigitDF_Test$Label))
## Make sure you take the labels out of the testing data
(head(DigitDF_Test))
DigitDF_Test_noLabel<-DigitDF_Test[-c(1)]
DigitDF_Test_justLabel<-DigitDF_Test$label
(head(DigitDF_Test_noLabel))
# decision tree modeling with Train
dt_Train <- rpart(label ~., data = DigitDF_Train, method="class", control=rpart.control(cp=0, minsplit=4, maxdepth=25))
rpart.plot(dt_Train, extra=104, box.palette="GnBu", cex=NULL, branch.lty=3, shadow.col="gray", nn=TRUE)
dt_Pred <- predict(dt_Train, DigitDF_Test_noLabel, type="class") # maybe withoutlabels?
#Testing accurancy of decision tree with Kaggle train data sub set
(confusionMatrix(dt_Pred, DigitDF_Test$label))
# Accumulate results from each fold, if you like
AllResultsDT <- c(AllResultsDT,dt_Pred)
AllLabelsDT <- c(AllLabelsDT, DigitDF_Test_justLabel)
}
proc.time() - startTime
### end crossvalidation -- present results for all folds
dt_table <- table(unlist(AllResultsDT),unlist(AllLabelsDT))
# test accuracy
print(paste('Accuracy for CV Decision Trees', sum(diag(dt_table)) / sum(dt_table)))
